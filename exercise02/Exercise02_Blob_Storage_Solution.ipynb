{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Exercise02_Blob_Storage_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buDUMbpKYTcD"
      },
      "source": [
        "# <center>Big Data &ndash; Exercises &ndash; Solution</center>\n",
        "## <center>Fall 2021 &ndash; Week 2 &ndash; ETH Zurich</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO44IswhYTcE"
      },
      "source": [
        "## Exercise 1: Storage devices (Optional)\n",
        "\n",
        "In this exercise, we want to understand the differences between [SSD](https://en.wikipedia.org/wiki/Solid-state_drive), [HDD](https://en.wikipedia.org/wiki/Hard_disk_drive), and [SDRAM](https://en.wikipedia.org/wiki/Synchronous_dynamic_random-access_memory) in terms of __capacity__, __speed__ and __price__. \n",
        "\n",
        "### Task 1\n",
        "Fill in the table below by visiting your local online hardware store and choosing the storage device with largest capacity available but optimizing for read/write speed.\n",
        "For instance, you can visit Digitec.ch to explore the prices on [SSDs](https://www.digitec.ch/en/s1/producttype/ssd-545?tagIds=76), [HDDs](https://www.digitec.ch/en/s1/producttype/hard-drives-36?tagIds=76), and \n",
        "[SDRAMs](https://www.digitec.ch/en/s1/producttype/memory-2?tagIds=76). \n",
        "You are free to use any other website for filling the table. \n",
        "\n",
        "\n",
        "| Storage Device | Maximum capacity, GB | Price, CHF/GB  | Read speed, GB/s | Write speed, GB/s | Link |\n",
        "| --------------:| --------------------:| --------------:|-----------------:|------------------:|------|\n",
        "| HDD            |                      |                |                  |                   |&nbsp;|\n",
        "| SSD            |                      |                |                  |                   |&nbsp;|\n",
        "| DRAM           |                      |                |                  |                   |&nbsp;|\n",
        "\n",
        "\n",
        "### Task 2\n",
        "Answer the following questions:\n",
        "1. What type of storage devices above is the cheapest one?\n",
        "2. What type of storage devices above is the fastest in terms of read speed?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLSwwJspYTcE"
      },
      "source": [
        "### Solution:\n",
        "\n",
        "Looking at Digitec, we fill the table as follows:\n",
        "\n",
        "| Storage Device | Maximum capacity, GB | Price, CHF/GB  | Read speed, GB/s | Write speed, GB/s | Link |\n",
        "| --------------:| --------------------:| --------------:|-----------------:|------------------:|------|\n",
        "| HDD            |    18 TB;            |   0.032 CHF/GB |      0.25 GB/s   | 0.25 GB/s          | [link1](https://www.digitec.ch/en/s1/product/seagate-ironwolf-pro-16tb-35-cmr-hard-drives-11205736?tagIds=76-535) [link2](https://www.seagate.com/de/de/internal-hard-drives/hdd/ironwolf/) |\n",
        "| SSD            |    4 TB;             |   0.25  CHF/GB |      7.3  GB/s   |      6.9 GB/s    | [link](https://www.digitec.ch/en/s1/product/seagate-firecuda-530-4000-gb-m2-2280-ssd-16172210) |\n",
        "| DRAM           |    128 GB per module; |  12.5   CHF/GB |     60   GB/s    |       48 GB/s     | [link1](https://www.digitec.ch/en/s1/product/samsung-m386a8k40bm1-crc-1x-64gb-ddr4-2133-dimm-288-memory-8989701) [link2](https://www.techspot.com/news/62129-ddr3-vs-ddr4-raw-bandwidth-numbers.html)|\n",
        "\n",
        "\n",
        "1. HDDs are the cheapest storage device among mentioned devices \n",
        "2. DRAMs are the fastest storage device among mentioned devices "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I9yZ93XYTcF"
      },
      "source": [
        "## Exercise 2: Seting up an Azure storage account\n",
        "\n",
        "In this section you'll learn how to set up a Locally Redundant Storage instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0xIdjVrYTcG"
      },
      "source": [
        "\n",
        "### Step 1: Create a Locally-Redundant Storage\n",
        "\n",
        "1. First, you need to create a storage account. In the Azure portal, click on the option \"Storage accounts\" in the left hand side menu. \n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/O50x6Ip3wAfHZZt/download\" width=800/>\n",
        "\n",
        "2. Click on the \"Add\" button at the top of the page. \n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/28ZnfQidRktKXNG/download\" width=800/>\n",
        "\n",
        "3. Fill in the form in the following way:\n",
        "  * If not already present, create a new resource group called *exercise02*\n",
        "  * Give the LRS a unique name\n",
        "  * Select *Locally-redundant storage (LRS)* as *Replication* mode\n",
        "  * The *Storage Account Name* can be whatever you want \n",
        "  * Leave all other values unchanged \n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/NxLQbGomz0tkPUd/download\" width=800/>\n",
        "\n",
        "4. Click *Review + create* then *Create* on the next page (deployment might take a few minutes).\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/XthQkrrg9PrEOSq/download\" width=800/>\n",
        "\n",
        "5. Go to the resource page of the newly created LRS.\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/A0rZoRbC7BJiPVA/download\" width=800/>\n",
        "\n",
        "6. In the left-hand menu, under the *Settings* group, select the *Access Keys* tab.\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/xrbKc7AlqDUwbSq/download\" width=800/>\n",
        "\n",
        "7. Copy one of the access keys to the clipboard. You might first need to click the *Show keys* button at the top of the page.  \n",
        "\n",
        "8. Paste the *Storage Account Name* in `ACCOUNT_NAME`, the access key in `ACCOUNT_KEY`, and add an arbitrary string in`CONTAINER_NAME` (or leave it as default)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fkn8l67-YTcG"
      },
      "source": [
        "ACCOUNT_NAME   = '...'\n",
        "ACCOUNT_KEY    = '...'\n",
        "CONTAINER_NAME = 'exercise02'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnk5IeGUYTcK"
      },
      "source": [
        "\n",
        "###  Step 2: Installing and Importing the Azure Storage Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYlT4mz0YTcK",
        "outputId": "9e127be6-fff1-4341-da14-529c1a2d9924"
      },
      "source": [
        "!pip install azure-storage==0.33.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-storage==0.33.0\n",
            "  Downloading azure_storage-0.33.0-py3-none-any.whl (181 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 30 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 40 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 61 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 71 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 81 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 92 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 102 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 112 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 122 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 133 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 143 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 153 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 163 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 174 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 181 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting azure-nspkg\n",
            "  Downloading azure_nspkg-3.0.2-py3-none-any.whl (1.5 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from azure-storage==0.33.0) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from azure-storage==0.33.0) (2.23.0)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 49.3 MB/s \n",
            "\u001b[?25hCollecting azure-common\n",
            "  Downloading azure_common-1.1.27-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->azure-storage==0.33.0) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->azure-storage==0.33.0) (2.20)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->azure-storage==0.33.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->azure-storage==0.33.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->azure-storage==0.33.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->azure-storage==0.33.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->azure-storage==0.33.0) (2.10)\n",
            "Installing collected packages: cryptography, azure-nspkg, azure-common, azure-storage\n",
            "Successfully installed azure-common-1.1.27 azure-nspkg-3.0.2 azure-storage-0.33.0 cryptography-35.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dNNDi16YTcO"
      },
      "source": [
        "from azure.storage.blob import BlockBlobService\n",
        "from azure.storage.blob import PageBlobService\n",
        "from azure.storage.blob import AppendBlobService\n",
        "from azure.storage.blob import PublicAccess\n",
        "from azure.storage.models import LocationMode\n",
        "from azure.storage.blob import ContentSettings\n",
        "from azure.storage.blob import BlockListType\n",
        "from azure.storage.blob import BlobBlock\n",
        "from timeit import default_timer as timer\n",
        "import uuid\n",
        "import random\n",
        "\n",
        "#function for genereting unique names for blobs\n",
        "def get_blob_name():\n",
        "    return '{}{}'.format('blob', str(uuid.uuid4()).replace('-', ''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KJE0basYTcR"
      },
      "source": [
        "## Exercise 3: Azure Blob Storage Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZhGPTbEYTcT"
      },
      "source": [
        "### Step 1: Explore Concepts of Azure Blob Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpcEciffYTcU"
      },
      "source": [
        "1. A container provides a grouping of a set of blobs. All blobs must be in a container. An account can contain an unlimited number of containers, and a container can store an unlimited number of blobs. Note that the container name must be lowercase.\n",
        "\n",
        "![Image of blob](https://docs.microsoft.com/en-us/azure/includes/media/storage-blob-concepts-include/blob1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8KTwZN6YTcU"
      },
      "source": [
        "2. Let us look at the different types of blobs available in Azure Blob storage by reading the article at the following [link](https://docs.microsoft.com/en-us/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs). After you are done, inspect the table below, and determine which type of blob is suitable for each of the use cases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We5Y38vKYTcV"
      },
      "source": [
        "|                | Block Blob | Append Blob  | Page Blob |\n",
        "| --------------:| -----------| ------------:| ---------:|\n",
        "| Static content delivery             |            |              |           |\n",
        "| As a disk for a VirtualMachine       |            |              |           |\n",
        "| Streaming video                      |            |              |           |\n",
        "| Log Files                     |            |             |          |\n",
        "| Social network events (e.g., uploading photos to Instagram)          |            |              |           |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfm2NR7mYTcV"
      },
      "source": [
        "__Solution:__\n",
        "\n",
        "|                | Block Blob | Append Blob  | Page Blob |\n",
        "| --------------:| -----------| ------------:| ---------:|\n",
        "| Static content delivery              |     X      |             |           |\n",
        "| As a disk for a VirtualMachine       |            |             |     X     |\n",
        "| Streaming video                      |     X      |             |           |\n",
        "| Log Files                     |            |       X      |           |\n",
        "| Social network events (e.g., uploading photos to Instagram) |     X      |              |           |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw2u-kWBYTcW"
      },
      "source": [
        "__Block blobs__ let you upload large blobs efficiently. Block blobs are comprised of blocks, each of which is identified by a block ID. You create or modify a block blob by writing a set of blocks and committing them by their block IDs. Each block can have a different size, up to a maximum of 4000 MB, and a block blob can include up to 50,000 blocks. The maximum size of a block blob is therefore slightly more than 190 TB (4000 MB X 50,000 blocks). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUfmdwkHYTcW"
      },
      "source": [
        "__Append blobs__ are comprised of blocks and are optimized for append operations. When one modifies an append blob, blocks are added to the end of the blob only - via the `append_block` operation -. Updating or deleting of existing blocks is not supported. Unlike a block blob, an append blob does not expose its block IDs.\n",
        "\n",
        "Each block in an append blob can have a different size, up to a maximum of 4 MB, and an append blob can include up to 50,000 blocks. The maximum size of an append blob is therefore slightly more than 195 GB (4 MB X 50,000 blocks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdibPyBxYTcX"
      },
      "source": [
        "__Page blobs__ are a collection of 512-byte pages optimized for random read and write operations. To create a page blob, you initialize the page blob and specify the maximum size the page blob will grow. To add or update the contents of a page blob, you write a page or a set of pages by specifying an offset and a range that align to 512-byte page boundaries. A write to a page blob can overwrite just one page, some pages, or up to 4 MB of the page blob. Writes to page blobs happen in-place and are immediately committed to the blob. The maximum size for a page blob is 8 TB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPx8UcHHYTcX"
      },
      "source": [
        "### Step 2: Test Your First Container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUm-3RJLYTcY"
      },
      "source": [
        "1. Create a new container under the specified account. If the container with the same name already exists, the operation fails and returns `False`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIGgxlRNYTcY",
        "outputId": "dd2901f7-cd33-4a8f-d7a3-283f7afaa62d"
      },
      "source": [
        "# Choose whether to have public access for this container\n",
        "public_access = False\n",
        "access_type = PublicAccess.Container if public_access else None\n",
        "\n",
        "# Create the container\n",
        "block_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
        "status = block_blob_service.create_container(CONTAINER_NAME, public_access=access_type)\n",
        "\n",
        "# Write a console message indicating if the container was successfully created\n",
        "if status==True:\n",
        "\tprint(f\"Container {CONTAINER_NAME} created\")\n",
        "else:\n",
        "\tprint(f\"Container {CONTAINER_NAME} already exists\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Container exercise02 created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DLdYvgpYTcc"
      },
      "source": [
        "2. Download a file to the Colab's Virtual Machine (VM)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTmDj-GYYTcc",
        "outputId": "515fda26-0a96-4332-9875-5f732831325c"
      },
      "source": [
        "!wget https://www.vetbabble.com/wp-content/uploads/2016/11/hiding-cat.jpg -O cat.jpg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-01 08:18:47--  https://www.vetbabble.com/wp-content/uploads/2016/11/hiding-cat.jpg\n",
            "Resolving www.vetbabble.com (www.vetbabble.com)... 172.66.40.63, 172.66.43.193, 2606:4700:3108::ac42:2bc1, ...\n",
            "Connecting to www.vetbabble.com (www.vetbabble.com)|172.66.40.63|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64847 (63K) [image/jpeg]\n",
            "Saving to: ‘cat.jpg’\n",
            "\n",
            "cat.jpg             100%[===================>]  63.33K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-10-01 08:18:47 (6.38 MB/s) - ‘cat.jpg’ saved [64847/64847]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lxjQ4LHYTcf"
      },
      "source": [
        "3. Upload the file to Azure Blob storage. Note that the name of the file on local machine (`local_file`) can differ from the name of the blob (`blob_name`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDYBc_XaYTcf",
        "outputId": "bb5da647-e484-4376-9f68-0c979a72778e"
      },
      "source": [
        "# Define the local and remote file names\n",
        "local_file = \"cat.jpg\"\n",
        "blob_name = \"picture\"\n",
        "\n",
        "# Create a blob which contains the downloaded image\n",
        "try:\n",
        "  block_blob_service.create_blob_from_path(\n",
        "    CONTAINER_NAME,\n",
        "    blob_name,\n",
        "    local_file,\n",
        "    content_settings=ContentSettings(content_type='image/jpg')\n",
        "  )\n",
        "  print(\"Blob URL:\", block_blob_service.make_blob_url(CONTAINER_NAME, blob_name))\n",
        "except:\n",
        "  print (\"Could not create the blob\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blob URL: https://sd1n2in12.blob.core.windows.net/exercise02/picture\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKKIlfDtYTch"
      },
      "source": [
        "4. Try to open the link above\n",
        "\n",
        "By default, the new container is private, so you must specify your storage access key (as you did earlier) to download blobs from this container. If you want to make the blobs within the container available to everyone, you can create the container and pass the public access level using the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbZ0RXlIYTch",
        "outputId": "394e15c8-5c86-467b-e56c-2f65808ec401"
      },
      "source": [
        "# Give your container public access\n",
        "block_blob_service.set_container_acl(CONTAINER_NAME, public_access=PublicAccess.Container)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<azure.storage.blob.models.ResourceProperties at 0x7f9c0b0bc350>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW-LKLxBYTck"
      },
      "source": [
        "After this change, anyone on the Internet can see blobs in a public container, but only you can modify or delete them. \n",
        "\n",
        "Try to open the link again. Note that it may take a few seconds to change access permisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_olUuU0YTck"
      },
      "source": [
        "5. List all blobs in the container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIjWL8E-YTcl"
      },
      "source": [
        "In order to list the blobs in a container, use the `list_blobs` method. This method returns a generator which can be iterated over in a loop. The following code outputs the name, type, size and url of each blob in a container."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO9QLSNvYTcm",
        "scrolled": true,
        "outputId": "cf1addcc-746f-469e-bc7e-c517fb2c6d58"
      },
      "source": [
        "# List all blobs in the container\n",
        "blobs = block_blob_service.list_blobs(CONTAINER_NAME)\n",
        "for blob in blobs:\n",
        "  try:\n",
        "    print(f\"Name: {blob.name}\") \n",
        "    print(f\" > Type: {blob.properties.blob_type}\") \n",
        "    print(f\" > Size: {blob.properties.content_length}\") \n",
        "    print(f\" > URL:  {block_blob_service.make_blob_url(CONTAINER_NAME,blob.name)}\")\n",
        "  except:\n",
        "    print(\"Something went wrong!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: picture\n",
            " > Type: BlockBlob\n",
            " > Size: 64847\n",
            " > URL:  https://sd1n2in12.blob.core.windows.net/exercise02/picture\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApIkQqNLYTcp"
      },
      "source": [
        "6. Download blobs\n",
        "\n",
        "In order to download data from a blob, use `get_blob_to_path`, `get_blob_to_stream`, `get_blob_to_bytes`, or `get_blob_to_text`. They are high-level methods that perform the necessary chunking when the size of the data exceeds 64 MB.\n",
        "\n",
        "Note: The name of the file after downloading can differ from the name of the blob.\n",
        "\n",
        "The following example uses `get_blob_to_path` to download the content of your container and store it with names of the form `file_i`, where `i` is a sequential index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oioem6vFYTcp",
        "scrolled": true,
        "outputId": "dde2f23f-bf3c-4e4a-bd5b-603a7e96b94c"
      },
      "source": [
        "# Specify the local path where the contents will be downloaded\n",
        "LOCAL_PATH = \".\"   \n",
        "\n",
        "# Iterate through the blobs and download them\n",
        "blobs = block_blob_service.list_blobs(CONTAINER_NAME)\n",
        "for i, blob in enumerate(blobs):\n",
        "  local_file = f\"{LOCAL_PATH}/file_{i}\"\n",
        "  try:\n",
        "    block_blob_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
        "    print(f\"Successfully downloaded {local_file}\")\n",
        "  except:\n",
        "    print(f\"Something went wrong while downloading blob {blob.name}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded ./file_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KWAxobvCRXc"
      },
      "source": [
        "Since the downloaded file does not have an extension, Colab will not know how to interpret it. The cell below adds the `.jpg` extension to the name. After this, try opening the file and see what you get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKobLW0nCfpG"
      },
      "source": [
        "!mv file_0 file_0.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuamEfU6YTct"
      },
      "source": [
        "### Step 3: Using the REST API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQStA0qAYTct"
      },
      "source": [
        "REpresentational State Transfer (__REST__) defines a set of constraints for internet-scale distributed systems. These constraints emphasize concepts such as scalability of interactions between the system's components, uniform interfaces, layered architecture and independence of deployment among components. __RESTful__ web services provide interoperability between computer systems on the Internet. REST-compliant web services allow the requesting systems to access and manipulate textual representations of web resources by using a uniform and predefined set of **stateless** operations.\n",
        "\n",
        "The best known RESTful protocol is HTTP. HTTP defines a set of Request Methods which are to be performed by an entity on the Internet on a resource identified by a URI. The best known methods in HTTP are GET, POST, PUT, DELETE. The response of such methods may be in XML, HTML, JSON, or some other format. \n",
        "\n",
        "You can find the Azure Blob Service API description at the following [link](https://docs.microsoft.com/en-us/rest/api/storageservices/blob-service-rest-api), and the HTTP response codes defined by the World Wide Web Consortium (W3C) [here](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEFbTp_4YTct"
      },
      "source": [
        "We could use tools like [Postman](https://www.getpostman.com/), CURL or others to make REST requests to Azure Storage. In this tutorial we will use [Reqbin](https://reqbin.com/), a simple effective website to post HTTP request online.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBR9pisaYTcu"
      },
      "source": [
        "### Tasks\n",
        "\n",
        "Complete the tasks below:\n",
        "\n",
        "1. Use any tool for listing all blobs in your container. For this, use the following [REST request](https://docs.microsoft.com/en-us/rest/api/storageservices/list-blobs). To avoid setting up an authentication you can make your container public by changing access policy to *Container*. See Pictures below:\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/rIw7dSK20PEH5um/download\" width=600/>\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/IpjOgJtOj2v13fT/download\" width=600/>\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/k5MVQE8jKQg6OOJ/download\" width=600/>\n",
        "\n",
        "Alternatively, you can also use the cell below to do this without accessing the Azure UI:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "YO4mdkqhKi4p",
        "outputId": "2a547691-57a9-468e-87ab-dcb45b8d0740"
      },
      "source": [
        "block_blob_service.set_container_acl(CONTAINER_NAME, public_access=PublicAccess.Container)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<azure.storage.blob.models.ResourceProperties at 0x7f84b31f1a20>"
            ]
          },
          "execution_count": 31,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRvRUH0EKml5"
      },
      "source": [
        "2. Explain why the request above does not include a **body** part.\n",
        "3. What is the response format of the request? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i17fZm5fYTcu"
      },
      "source": [
        "__Solutions:__\n",
        "\n",
        "1. Listing all blobs in a container:\n",
        "\n",
        "__Solution 1:__ Using Reqbin.\n",
        "\n",
        "You can use as URL the following, but replacing the storage account name with what you chose instead of `myaccount` and the container we set up in this exercise instead of `mycontainer`. <br>\n",
        "\n",
        "https://myaccount.blob.core.windows.net/mycontainer?restype=container&comp=list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmdnK9BKYTcv"
      },
      "source": [
        "__Solution 2:__ Using `curl`.\n",
        "\n",
        "As specified above, make sure to replace `myaccount` and `mycontainer` with the values specific to your setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "PpDzZx_JYTcv",
        "outputId": "c747d2dc-6a9d-412d-b2ca-d8a845e92368"
      },
      "source": [
        "!curl \"https://myaccount.blob.core.windows.net/mycontainer?restype=container&comp=list\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "﻿<?xml version=\"1.0\" encoding=\"utf-8\"?><EnumerationResults ContainerName=\"https://hnosndoih192n1209.blob.core.windows.net/exercise02\"><Blobs><Blob><Name>picture</Name><Url>https://hnosndoih192n1209.blob.core.windows.net/exercise02/picture</Url><Properties><Last-Modified>Mon, 21 Sep 2020 13:57:44 GMT</Last-Modified><Etag>0x8D85E36503CDF62</Etag><Content-Length>64847</Content-Length><Content-Type>image/jpg</Content-Type><Content-Encoding /><Content-Language /><Content-MD5>hVQNOkYiJfAETezlIZcMlQ==</Content-MD5><Cache-Control /><BlobType>BlockBlob</BlobType><LeaseStatus>unlocked</LeaseStatus></Properties></Blob></Blobs><NextMarker /></EnumerationResults>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJgo7n9_YTcx"
      },
      "source": [
        "2. We are perforing a **GET** request, which does not have a body.\n",
        "3. The format of the response is **XML**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJNn-SViYTcy"
      },
      "source": [
        "# Exercise 4. KeyValue Vector Clocks\n",
        "\n",
        "As pointed out in the lecture, the concepts are clearly explained in the Dynamo paper by the DeCandia, G., et. al. (2007). \"Dynamo: Amazon’s Highly Available Key-value Store\". In SOSP ’07 (Vol. 41, p. 205). [DOI](https://dl.acm.org/citation.cfm?doid=1294261.1294281)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcHVf7fIYTcy"
      },
      "source": [
        "## Task 1\n",
        "Multiple distributed hash tables use vector clocks for capturing causality among different versions of the same object. In Amazon's Dynamo, a vector clock is associated with every version of every object.\n",
        "\n",
        "Let $VC$ be an $N$-element array which contains non-negative integers, initialized to 0, representing $N$ logical clocks of the $N$ processes (nodes) of the system. $VC$ gets its $j$ element incremented by one everytime node $j$ performs a write operation on it. <br>\n",
        "Moreover, $VC(x)$ denotes the vector clock of a write event, and $VC(x)_z$ denotes the element of that clock for the node $z$.\n",
        "\n",
        "Try to __formally define__ the partial ordering that we get from using vector clocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5ihYlHhYTcy"
      },
      "source": [
        "__Solution__:\n",
        "\n",
        "$VC(x) \\leq VC(y) \\iff \\forall z[VC(x)_z \\leq VC(y)_z]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4lpxnbaYTcz"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "Vector clock antisymmetry property is defined as follows:\n",
        "\n",
        "If $ VC(x) \\lt VC(y)$, then $ ¬ \\ (VC(y) \\lt VC(x)) $\n",
        "\n",
        "Prove this property."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhhkSKQ3YTcz"
      },
      "source": [
        "__Solution:__\n",
        "\n",
        "We will prove the above statement using proof by contradiction.\n",
        "\n",
        "Based on the fundamental definitions of vector clocks, we can expand $VC(x) \\lt VC(y)$ into:\n",
        "\n",
        "$VC(x) \\lt VC(y) \\iff \\forall z[VC(x)_z \\leq VC(y)_z] \\wedge \\exists z'[VC(x)_{z'} \\lt VC(y)_{z'}]$\n",
        "\n",
        "We begin our proof by assuming $VC(x) \\lt VC(y)$ is true. This means that there is a $z'$ element for which $VC(x)_{z'} \\lt VC(y)_{z'}$ holds.\n",
        "\n",
        "We now assume that $VC(y) \\lt VC(x)$ is also true. This implies that $\\forall z[VC(y)_z \\leq VC(x)_z] \\wedge \\exists k'[VC(y)_{k'} \\lt VC(x)_{k'}]$. However this is a contradiction, since the there exists a $z'$ for which $VC(x)_{z'} \\lt VC(y)_{z'}$. This disproves the conjunction, which concludes our proof."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6aDCNvtYTc0"
      },
      "source": [
        "## Task 3\n",
        "\n",
        "Consider $j$ servers in a cluster where $S_j$ denotes the $j$th node.  \n",
        "In this exercise, we adopt a slightly modified notation from the Dynamo paper:  \n",
        "- The Dynamo paper indicates the writing server on the edge, we however write it before the colon.  \n",
        "- For brevity, we index server by position and omit server name in the vector clock.\n",
        "\n",
        "For example **aa ([$S_0$,0],[$S_1$,4])** with $S_1$ as writing server become **$S_1$ : aa ([0,4])**\n",
        "So, given the following version evolution DAG for a particular object, complete the vector clocks computed at the corresponding version.\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/iRONxqhpQkRdLeY/download\" width=400/>\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/WzJlMxIrA2RGcKh/download\" width=400/>\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/nZ83Jb7mrr0uhi8/download\" width=400/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rdvxg65YTc0"
      },
      "source": [
        "__Solution:__\n",
        "\n",
        "Note that for the missing written values, any value is valid. For instance, in *Obj. 2*, for the last write done by $S_2$, `mq` or `1234` would've been just as valid as `pp`.\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/YFMitA0UCJM5Fer/download\" width=900/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggoaIglcYTc1"
      },
      "source": [
        "## Task 4\n",
        "\n",
        "When a get request comes in to Amazon Dynamo with some key, then:\n",
        "  - The coordinator node (selected from the preference list as the top node for this key) is taking care of this request\n",
        "  - The coordinator node requests from other nodes (itself + the next N-1 healthy ones on the preference list), and receives, a set of versions for the value associated with the key, that are modelled as __value (vector clock)__ pairs such as a ([1, 3, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA4DaMfGYTc1"
      },
      "source": [
        "### Task 4.1\n",
        "Given the following list of versions, draw the version DAG that the coordinator node will build for returning available versions.\n",
        "\n",
        "1 ([0,0,1])  \n",
        "1 ([0,1,1])  \n",
        "2 ([1,1,1])  \n",
        "3 ([0,2,1])  \n",
        "10 ([1,3,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08D9-zKHYTc2"
      },
      "source": [
        "__Solution:__\n",
        "We could use the following rules as guidance for creating correct version DAGs. \n",
        "\n",
        "- If no partial ordering exists between nodes, then those nodes should be on the same level, i.e., they are both valid versions.\n",
        "- If there is an edge between two nodes, then the parent node should be smaller than the child.\n",
        "- You cannot have skip connections, i.e., there cannot be an edge from an ancestor node (excluding the parent node) directly to a child node.\n",
        "- Transitive edges shouldn't be present in the version DAG.\n",
        "- Each edge represents the update of exactly one entry of the vector clock.\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/MfhAFv3eIM2TPmJ/download\" width=400/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1qfPU1BYTc3"
      },
      "source": [
        "### Task 4.2\n",
        "Given the following list of versions, draw the version DAG that the coordinator node will build for returning the correct version.\n",
        "\n",
        "\n",
        " a ([1,0,0])  \n",
        " b ([0,1,0])  \n",
        " c ([2,1,0])   \n",
        " d ([2,1,1])   \n",
        " e ([3,1,1])  \n",
        " f ([2,2,1])   \n",
        " g ([3,1,2])   \n",
        " h ([3,2,3])  \n",
        " i ([4,2,2])   \n",
        " j ([5,2,2])  \n",
        " k ([4,3,3])  \n",
        " l ([5,2,3])  \n",
        " m ([5,4,3])  \n",
        " n ([6,3,3])  \n",
        " o ([6,4,4])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucKC8J3mYTc3"
      },
      "source": [
        "__Solution:__\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/JM008TpUs4it7dq/download\" width=300/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CglmCvJYTc4"
      },
      "source": [
        "### Task 4.3\n",
        "Given the following list of versions, draw the version DAG that the coordinator node will build for returning the correct version.\n",
        "\n",
        "a ([1,0,0,0])  \n",
        "b ([0,0,0,1])  \n",
        "aa ([0,0,1,0])  \n",
        "bb ([0,1,0,0])  \n",
        "c ([1,2,0,1])  \n",
        "cc ([0,1,1,2])  \n",
        "d ([1,3,0,1])  \n",
        "f ([1,2,1,3])  \n",
        "e ([2,1,1,2])  \n",
        "g ([2,2,2,3])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yek61oybYTc4"
      },
      "source": [
        "__Solution:__\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/1VdCzInpVG9xZtb/download\" width=700/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox_oCR30YTc5"
      },
      "source": [
        "## Task 5 (Optional)\n",
        "\n",
        "Consider $j$ servers in a cluster where $S_j$ denotes the $j$th node. The following table denotes the execution of a series of get/put operations. Also each line of the table represents the events that happen at the time time. For example, at time 0 (`t0`) servers $S_1$ and $S_3$ perform operations. Moreover, when reading and writing an object, we are provided with / must provide a context respecitvely. The context itself is the vector clock, and helps the routines understand what version of the object they are dealing with, and what the new, updated version of the context will be.\n",
        "\n",
        "For the `get` and `put` routines, we have the following signatures:\n",
        "\n",
        "* `get(key)` $\\rightarrow$ `[val_1, val_2, ...]`, $C_{key}$ `(` $VC$ `(key))` \n",
        "  * Example: `get(\"foo\")` $\\rightarrow$ `[bar_1, bar_2]`, $C_{2}$ `([1, 0, 1, 0]) # We assume the existence of 4 nodes` \n",
        "\n",
        "* `put(key, context, val)` $\\rightarrow$ `None`\n",
        "  * Example: `put(\"foo\",` $C_2$`, \"bar\")`  \n",
        "\n",
        "Note that the $C_{key}$ elements are just notation, and are meant to highlight that the context gets passed around between the `get` and `put` routines in a real API. \n",
        "\n",
        "Complete missing `[list_values],` $C_{key}$ `([vector_clock])` tuples for the calls below.\n",
        "\n",
        "<table>\n",
        "  <tr><th></th><th>S0</th><th>S1</th><th>S2</th><th>S3</th><th>S4</th></tr>\n",
        "  <tr>\n",
        "    <td>t0</td>\n",
        "    <td></td>\n",
        "    <td>Get(1)$\\rightarrow$ _______________, $C_{1}$(_______________)<br>Put(1, _____, ”a”)</td>\n",
        "    <td></td>\n",
        "    <td>Get(1)$\\rightarrow$ _______________, $C_{2}$(_______________)<br>Put(1, _____, ”bb”)</td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>t2</td>\n",
        "    <td>Get(1)$\\rightarrow$ _______________, $C_4$(_______________)<br>Put (1, _____, “rr”)</td>\n",
        "    <td>Get(1)$\\rightarrow$ _______________, $C_5$(_______________)<br>Put (1, _____, ”dd”)\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>t4</td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td>Get(1)$\\rightarrow$ _______________, $C_9$(_______________) <br>Put(1, _____, ”ccc”)</td>\n",
        "    <td>Get(1)$\\rightarrow$ _______________, $C_{10}$(_______________) <br> Put(1, _____, ”dd”)</td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>t5</td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td>Get(1)$\\rightarrow$ _______________, $C_{11}$(_______________) <br>Put(1, _____, “fff”)</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "The DAG below shows the interaction among nodes when retrieving values. You can use it for determining the expected values.\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/YoWi7QK2DcMHJFe/download\" width=400/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssSIZECNYTc5"
      },
      "source": [
        "__Solution:__\n",
        "<table>\n",
        "  <tr><th></th><th>S0</th><th>S1</th><th>S2</th><th>S3</th><th>S4</th></tr>\n",
        "  <tr>\n",
        "    <td>t0</td>\n",
        "    <td></td>\n",
        "    <td>Get(1)$\\rightarrow [], C_1$()<br>Put(1, $C_1$, ”a”)</td>\n",
        "    <td></td>\n",
        "    <td>Get(1)$\\rightarrow [], C_2$()<br>Put(1, $C_2$, ”bb”)</td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>t2</td>\n",
        "    <td>Get(1)$\\rightarrow [a, bb], C_4$([0,1,0,1,0])<br>Put (1, $C_4$, “rr”)</td>\n",
        "    <td>Get(1)$\\rightarrow [a, bb], C_5$([0,1,0,1,0])<br>Put (1, $C_5$, ”dd”)</td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>t4</td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td>Get(1)$\\rightarrow [rr], C_9$([1,1,0,1,0]) <br>Put(1, $C_9$, ”ccc”)</td>\n",
        "    <td>Get(1)$\\rightarrow [dd], C_{10}$([0,2,0,1,0]) <br> Put(1, $C_{10}$, ”dd”)</td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>t5</td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td></td>\n",
        "    <td>Get(1)$\\rightarrow [ccc, dd], $<br>$C_{11}$([1,2,1,2,0]) <br> Put(1, $C_{11}$, “fff”)</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXhEc_lZYTc5"
      },
      "source": [
        "# Exercise 5. Merkle Trees\n",
        "A hash tree or Merkle tree is a binary tree in which every leaf node gets as its label a data block and every non-leaf node is labelled with the cryptographic hash of the labels of its child nodes. \n",
        "\n",
        "Some KeyValue stores use Merkle trees for efficiently detecting inconsistencies in data between replicas. \n",
        "\n",
        "This works by exchaging first the root hash, comparing it with their own. If the hashes match, the replicas are synchronised. If they do not match, then the children of the node (in the Merkle tree) will be retrieved, and their hashes will be compared. This process continues until the inconsistent leave(s) are identified. \n",
        "\n",
        "## Task 1\n",
        "The two pictures below depict two Merkle trees each one belonging to two different replicas. Both should represent the same object.\n",
        "\n",
        "For the two pairs of trees below. Specify if it is a possible configuration as well as which nodes have to be exchanged in order to sync the trees, if applicable.  \n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/vqj7AOAozZKEO3N/download\" width=800/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt7NyfLzF73t"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7-gSY9IYTc6"
      },
      "source": [
        "__Solution Pair I:__\n",
        "- Node A (root hash) is exchanged. Node A is not the same.\n",
        "- Fetch Node A children. \n",
        "- Nodes B and C are exchanged. Node B is the same so its children are not exchanged, but node C is not.\n",
        "- Fetch Node C children.\n",
        "- Nodes F and G are exchanged. Node F is the same so its children are not exchanged, but node G is not.\n",
        "- Fetch Node G children.\n",
        "- Nodes N and O are exchanged. They both differ, so data blocks represented by the leaf's hashes are expected to be different (which is indeed the case)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoLvKmfEdsz3"
      },
      "source": [
        "## Task 2\n",
        "Repeat the exercise for the following pair of Merkle Trees.\n",
        "\n",
        "<img src=\"https://polybox.ethz.ch/index.php/s/TwFd3KDxTrqq2B1/download\" width=800/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD3wBjxeYTc6"
      },
      "source": [
        "__Solution Pair II:__\n",
        "\n",
        "This configuration is theoretically impossible with a sounds hash function, since node `A` has different values in the trees while having exactly the same children. This would imply that our hash function can give two different results from the exact same input values.\n",
        "\n",
        "Another problem of the trees can be seen in nodes `C`, which have matching hashes, while the nodes `G` have different hashes across the two replicas. This is theoretically possible since hash functions generally have non-zero collision probability (i.e. the same hash value can be obtained from two different input datums). While this is an anomaly, and generally does not happen in practice with a strong enough hash, it can happen in theory, yielding undesirable results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZSykiQzYTc6"
      },
      "source": [
        "# Exercise 6. Virtual nodes\n",
        "\n",
        "Virtual nodes were introduced to avoid assigning data in an unbalanced manner and coping with hardware heterogeneity by taking into consideration the physical capacity of each server\n",
        "\n",
        "Let assume we have ten servers ($i_1$ to $i_{10}$) each with the following amount of main memory: `8GB, 16GB, 32GB, 8GB, 16GB, 0.5TiB, 1TiB, 0.25TiB, 10GB, 20GB`. Calculate the number of virtual nodes/tokens each server should get according to its main memory capacity if we want to have a total of `256` virtual nodes/tokens.\n",
        "\n",
        "Just for the purpose of the exercises if you get a fractional number of virtual nodes, always round up, even if the total sum of nodes in the end exceeds `256`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvmrZ9hFYTc7"
      },
      "source": [
        "__Solution:__\n",
        "\n",
        "The total amount of main memory available is: `1902 GBs = 8 + 16 + 32 + 8 + 16 + 512 + 1024 + 256 + 10 + 20`.\n",
        "\n",
        "So if we want to have 256 virtual nodes/tokens, each such virtual node should get approximately `7.42 GBs`. Thus, each physical server $i_1$ to $i_{10}$ should get around: `2, 3, 5, 2, 3, 69, 138, 35, 2, 3` number of virtual nodes/tokens. Note that due to the rounding up process, the total number of allocated virtual nodes would be `262`.\n",
        "\n",
        "In general, rounding up to the next integer number or rounding down to the previous integer number is not determinant as virtual nodes are logically positioned in random parts of the ring anyway. \n",
        "\n",
        "However, one thing to be taken into consideration in practice is that the server with the smallest capacity should have more than a single virtual node. This is because it might be \"unlucky\" and have to handle a large domain of responsability."
      ]
    }
  ]
}