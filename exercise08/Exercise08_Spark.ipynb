{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycwd1jWj2E8T"
   },
   "source": [
    "# <center>Big Data &ndash; Exercises </center>\n",
    "## <center>Fall 2021 &ndash; Week 8 &ndash; ETH Zurich</center>\n",
    "## <center>YARN + Spark</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69CsbKob2E8U"
   },
   "source": [
    "**WHEN OPENING THIS NOTEBOOK SELECT NO KERNEL IF THE INTENDED ONE IS NOT FOUND (PYSPARK3)**\n",
    "Reading: \n",
    "- Zaharia, M. et al. (2012). Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing. In NSDI. \n",
    "- Armbrust, M., et al. (2015). Spark SQL: Relational Data Processing in Spark. In SIGMOD. \n",
    "\n",
    "This exercise will consist of 2 main parts: \n",
    "* YARN architecture and theory \n",
    "* Hands-on practice with Spark on docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdbi0gZS2E8V"
   },
   "source": [
    "# Exercise 1 &mdash; What is YARN?\n",
    "\n",
    "Fundamentally, “**Y**et **A**nother **R**esource **N**egotiator”. **YARN**  is a resource scheduler designed to work on existing and new Hadoop clusters. \n",
    "\n",
    "YARN supports pluggable schedulers. The task of the scheduler is to share the resources of a large cluster among different tenants (applications) while trying to meet application demands (memory, CPU). A user may have several applications active at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNt4v5cG2E8V"
   },
   "source": [
    "### 1.1 &ndash; List at least 3 main shortcomings of MapReduce v1, which are addressed by YARN design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WJJEGE12E8X"
   },
   "source": [
    "### 1.2 &ndash; State which of the following statements are true:\n",
    "\n",
    "1. The ResourceManager has to provide fault tolerance for resources across the cluster \n",
    "\n",
    "1. Container allocation/deallocation can take place in a dynamic fashion as the application progresses. \n",
    "\n",
    "1. YARN plans to allow applications to only request resources in terms of memory usage and number of CPUs.\n",
    "\n",
    "1. Communications between the ResourceManager and NodeManagers are heartbeat-based. \n",
    "\n",
    "1. The ResourceManager does not have a global view of all usage of cluster resources. Therefore, it tries to make better scheduling decisions based on probabilistic prediction. \n",
    "\n",
    "1. ResourceManager has the ability to request resources back from a running application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGYiOFom2E8Y"
   },
   "source": [
    "### 1.3 &ndash; Whose responsibility is it? Say which component of YARN is resposible for each of the following tasks.\n",
    "\n",
    "1. Fault Tolerance of running applications *[ResourceManager | ApplicationMaster | NodeManager ]*\n",
    "1. Asking for resources needed for an application *[ResourceManager | ApplicationMaster | NodeManager ]*\n",
    "\n",
    "1. Providing leases to use containers *[ResourceManager | ApplicationMaster | NodeManager]*\n",
    "\n",
    "1. Tracking status and progress of running applications *[ResourceManager | ApplicationMaster | NodeManager]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQTrigNm2E8Z"
   },
   "source": [
    "### 1.4 &ndash; What is the typical configuration for YARN? Choose for the following components how many instances of them there are in a cluster.\n",
    "\n",
    "```\n",
    "1. ResourceManager                  a. One per cluster\n",
    "\n",
    "2. ApplicationMaster                b. One per node\n",
    "\n",
    "3. NodeManager                      c. Many per cluster, but usually not per every node\n",
    "\n",
    "4. Container                        d. Many per node \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df9Arowc2E8e"
   },
   "source": [
    "## 2. Setup the Spark cluster on Docker\n",
    "\n",
    "1. Start docker <br> \n",
    "   docker-compose up -d\n",
    "\n",
    "2. Access jupyter notebook <br> \n",
    "   http://localhost:8888/lab/tree/work/Exercise08_Spark.ipynb\n",
    "\n",
    "3. Download datasets\n",
    "- fruits      :<br> \n",
    "  ```wget \"https://cloud.inf.ethz.ch/s/YgDaSGbPktFyQMr/download/fruits.txt\"```\n",
    "- yellowthings:<br> \n",
    "  ```wget \"https://cloud.inf.ethz.ch/s/Fw3R9k9AS9KjYyz/download/yellowthings.txt\"```\n",
    "\n",
    "4. copy the data to hdfs :<br> \n",
    "  ```docker cp fruits.txt jupyter:/home/jovyan/work``` <br>\n",
    "  ```docker cp yellowthings.txt jupyter:/home/jovyan/work``` <br>\n",
    "(Copying the data to hdfs needs to be done only once and it might take 1-2 minutes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sj15DNuA2E8f"
   },
   "source": [
    "## 3. Apache Spark Architecuture\n",
    "\n",
    "Spark is a cluster computing platform designed to be fast and general purpose. Spark extends the MapReduce model to efficiently cover a wide range of workloads that previously required separate distributed systems, including interactive queries and stream processing. Spark offers the ability to run computations in memory.\n",
    "\n",
    "At a high level, every Spark application consists of a **driver program** that launches various parallel operations on a cluster. The driver program contains your application's main function and defines distributed datasets on the cluster, then applies operations to them.\n",
    "\n",
    "Driver programs access Spark through a **SparkContext** object, which represents a connection to a computing cluster. There is no need to create a SparkContext; it is created for you automatically when you run the first code cell in the Jupyter\n",
    "\n",
    "The driver communicates with a potentially large number of distributed workers called **executors**. The driver runs in its own process and each executor is a separate process. A driver and its executors are together termed a Spark application.\n",
    "\n",
    "![Image of Account](http://spark.apache.org/docs/latest/img/cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6K2sJlR2E8f"
   },
   "source": [
    "### 3.1 Understand resilient distributed datasets (RDD)\n",
    "\n",
    "An RDD in Spark is simply an immutable distributed collection of objects. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \n",
    "\n",
    "##### What are RDD operations?\n",
    "RDDs offer two types of operations: **transformations** and **actions**.\n",
    "\n",
    "* **Transformations** create a new dataset from an existing one. Transformations are lazy, meaning that no transformation is executed until you execute an action.\n",
    "* **Actions** compute a result based on an RDD, and either return it to the driver program or save it to an external storage system (e.g., HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su_8K5JS2E8g"
   },
   "source": [
    "Transformations and actions are different because of the way Spark computes RDDs. Although you can define new RDDs any time, Spark computes them only in a **lazy** fashion, that is, the first time they are used in an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zVeNxBR2E8g"
   },
   "source": [
    "##### Create Spark session (Spark install within jupyter docker image) and context\n",
    "\n",
    "RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from the sample data files that have been copied to hdfs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/17 15:56:43 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.storage.StorageUtils$\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint.<init>(BlockManagerMasterEndpoint.scala:110)\n\tat org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:348)\n\tat org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:287)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:336)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:191)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/bs/8kmw91_s5rg573ntbb11nw640000gn/T/ipykernel_22513/1341926732.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mspark\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaster\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'local'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkContext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/BigData/lib/python3.8/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    226\u001B[0m                             \u001B[0msparkConf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    227\u001B[0m                         \u001B[0;31m# This SparkContext may be an existing one.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 228\u001B[0;31m                         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msparkConf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    229\u001B[0m                     \u001B[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    230\u001B[0m                     \u001B[0;31m# by all sessions.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/BigData/lib/python3.8/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[0;34m(cls, conf)\u001B[0m\n\u001B[1;32m    390\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    391\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 392\u001B[0;31m                 \u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    393\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    394\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/BigData/lib/python3.8/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    144\u001B[0m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 146\u001B[0;31m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001B[0m\u001B[1;32m    147\u001B[0m                           conf, jsc, profiler_cls)\n\u001B[1;32m    148\u001B[0m         \u001B[0;32mexcept\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/BigData/lib/python3.8/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m_do_init\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    208\u001B[0m         \u001B[0;31m# Create the Java SparkContext through Py4J\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 209\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mjsc\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_initialize_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    210\u001B[0m         \u001B[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    211\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_jconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/BigData/lib/python3.8/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m_initialize_context\u001B[0;34m(self, jconf)\u001B[0m\n\u001B[1;32m    327\u001B[0m         \u001B[0mInitialize\u001B[0m \u001B[0mSparkContext\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfunction\u001B[0m \u001B[0mto\u001B[0m \u001B[0mallow\u001B[0m \u001B[0msubclass\u001B[0m \u001B[0mspecific\u001B[0m \u001B[0minitialization\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m         \"\"\"\n\u001B[0;32m--> 329\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mJavaSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    330\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    331\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mclassmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/BigData/lib/python3.8/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1571\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1572\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1573\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1574\u001B[0m             answer, self._gateway_client, None, self._fqn)\n\u001B[1;32m   1575\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/BigData/lib/python3.8/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.storage.StorageUtils$\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint.<init>(BlockManagerMasterEndpoint.scala:110)\n\tat org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:348)\n\tat org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:287)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:336)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:191)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zVeNxBR2E8g"
   },
   "source": [
    "##### How do I make an RDD?\n",
    "\n",
    "RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from the sample data files that have been copied to hdfs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2390.4140625,
      "end_time": 1.604569551027697E12
     }
    },
    "id": "coRjk8iA2E8g",
    "outputId": "95d2ab62-4891-48a2-ddc9-cd11fa42286a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/bs/8kmw91_s5rg573ntbb11nw640000gn/T/ipykernel_22513/3440523324.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# sc is the Spark Context object automatically created for you\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mfruits\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtextFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'fruits.txt'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0myellowThings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtextFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'yellowthings.txt'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# sc is the Spark Context object automatically created for you\n",
    "fruits = sc.textFile('fruits.txt')\n",
    "yellowThings = sc.textFile('yellowthings.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APVFv9Ps2E8j"
   },
   "source": [
    "##### RDD transformations\n",
    "Following are examples of some of the common transformations available. For a detailed list, see [RDD Transformations](https://spark.apache.org/docs/2.0.0/programming-guide.html#transformations)\n",
    "\n",
    "Run some transformations below to understand this better. **Note that in the cells below, we're using the `collect()` method. This is in fact an *action*, not a *transformation*, however it's being used in these cells as a means of materializing the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 309.608154296875,
      "end_time": 1.604570982369364E12
     }
    },
    "id": "atWh0L_e2E8k",
    "outputId": "aaa2337e-0d43-4f03-e90c-531c3c2652ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elppa',\n",
       " 'ananab',\n",
       " 'nolem yranac',\n",
       " 'eparg',\n",
       " 'nomel',\n",
       " 'egnaro',\n",
       " 'elppaenip',\n",
       " 'yrrebwarts']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map\n",
    "fruitsReversed = fruits.map(lambda fruit: fruit[::-1])\n",
    "fruitsReversed.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 261.870849609375,
      "end_time": 1.604570982657437E12
     }
    },
    "id": "PT7_eUQR2E8m",
    "outputId": "ce3919e0-99c1-44db-ec38-a9bdb79d13f3"
   },
   "outputs": [],
   "source": [
    "# filter\n",
    "shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5)\n",
    "shortFruits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 289.953857421875,
      "end_time": 1.604570982970662E12
     }
    },
    "id": "Oi_FWVI_2E8o",
    "outputId": "d0cfae93-ee9e-4151-9b33-f385448b32ca"
   },
   "outputs": [],
   "source": [
    "# flatMap\n",
    "characters = fruits.flatMap(lambda fruit: list(fruit))\n",
    "characters.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 820.903076171875,
      "end_time": 1.604570983811643E12
     }
    },
    "id": "wRKKpIkl2E8q",
    "outputId": "2c4ae0f0-aeb1-499b-8186-12c4de8b8f39"
   },
   "outputs": [],
   "source": [
    "# union between fruits and yellowthings datasets\n",
    "fruitsAndYellowThings = fruits.union(yellowThings)\n",
    "fruitsAndYellowThings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 810.952880859375,
      "end_time": 1.604570984650106E12
     }
    },
    "id": "EsrYtoro2E8s",
    "outputId": "264fe33f-25e3-448a-d6f6-234fe1183bd3"
   },
   "outputs": [],
   "source": [
    "# intersection between fruits and yellowthings datasets\n",
    "yellowFruits = fruits.intersection(yellowThings)\n",
    "yellowFruits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 777.156005859375,
      "end_time": 1.604570985454588E12
     }
    },
    "id": "fq8rLaoc2E8u",
    "outputId": "c63df4eb-5d92-4181-9eb3-4da8ecea9dcc"
   },
   "outputs": [],
   "source": [
    "# distinct elements in the two datasets\n",
    "distinctFruitsAndYellowThings = fruitsAndYellowThings.distinct()\n",
    "distinctFruitsAndYellowThings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 800.93798828125,
      "end_time": 1.604570986276183E12
     }
    },
    "id": "cRGqpVgO2E8w",
    "outputId": "e18e313f-86e7-4e42-bf67-af5ac4720ab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For letter b\n",
      " >  banana\n",
      " >  bee\n",
      " >  butter\n",
      "For letter c\n",
      " >  canary melon\n",
      "For letter g\n",
      " >  gold\n",
      "For letter l\n",
      " >  lemon\n",
      "For letter p\n",
      " >  pineapple\n",
      "For letter s\n",
      " >  sunflower\n"
     ]
    }
   ],
   "source": [
    "# groupByKey\n",
    "yellowThingsByFirstLetter = yellowThings.map(lambda thing: (thing[0], thing)).groupByKey()\n",
    "for letter, lst in yellowThingsByFirstLetter.collect():\n",
    "  print(\"For letter\", letter)\n",
    "  for obj in lst:\n",
    "      print(\" > \", obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 777.8681640625,
      "end_time": 1.604570987091685E12
     }
    },
    "id": "YUjWL3Jp2E8y",
    "outputId": "df04cb76-bbc6-44e8-ca79-ec7809e12006"
   },
   "outputs": [],
   "source": [
    "# reduceByKey; key is the number of characters of the fruit name (len(fruit))\n",
    "numFruitsByLength = fruits.map(lambda fruit: (len(fruit), 1)).reduceByKey(lambda x, y: x + y)\n",
    "numFruitsByLength.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MIj-ytt2E8z"
   },
   "source": [
    "##### RDD actions\n",
    "Following are examples of some of the common actions available. For a detailed list, see [RDD Actions](https://spark.apache.org/docs/2.0.0/programming-guide.html#actions).\n",
    "\n",
    "Run some transformations below to understand this better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 279.06396484375,
      "end_time": 1.604570987396644E12
     }
    },
    "id": "rwqMc4iZ2E80",
    "outputId": "6409fad4-7979-4e9d-c83b-44238373872c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'canary melon', 'grape', 'lemon', 'orange', 'pineapple', 'strawberry']\n",
      "['banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower']\n"
     ]
    }
   ],
   "source": [
    "# collect\n",
    "fruitsArray = fruits.collect()\n",
    "yellowThingsArray = yellowThings.collect()\n",
    "print(fruitsArray)\n",
    "print(yellowThingsArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 281.281982421875,
      "end_time": 1.604570987702166E12
     }
    },
    "id": "UvyrLdxN2E82",
    "outputId": "845b0f07-3743-407a-9192-5cd9bff9063d"
   },
   "outputs": [],
   "source": [
    "# count - how many fruits are\n",
    "numFruits = fruits.count()\n",
    "numFruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 312.892822265625,
      "end_time": 1.604570988042097E12
     }
    },
    "id": "j274FPSm2E84",
    "outputId": "3e3ce495-e519-47c2-cc1c-4ae6b0a2cc3d"
   },
   "outputs": [],
   "source": [
    "# take - show the first three fruits\n",
    "first3Fruits = fruits.take(3)\n",
    "first3Fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 282.5498046875,
      "end_time": 1.604570988345706E12
     }
    },
    "id": "g5DOneF42E86",
    "outputId": "c897dc70-1691-4f2f-b46d-a52271eccfd3"
   },
   "outputs": [],
   "source": [
    "# reduce - what letters are used\n",
    "letterSet = fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y))\n",
    "letterSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nSN1JhN2E88"
   },
   "source": [
    "##### Lazy evaluation\n",
    "Lazy evaluation means that when we call a transformation on an RDD (for instance, calling `map()`), the operation is not immediately performed. Instead, Spark internally records metadata to indicate that this operation has been requested. Rather than thinking of an RDD as containing specific data, it is best to think of each RDD as\n",
    "consisting of instructions on how to compute the data that we build up through transformations. Loading data into an RDD is lazily evaluated in the same way transformations are. So, when we call `sc.textFile()`, the data is not loaded until it is necessary. As with transformations, the operation (in this case, reading the data) can\n",
    "occur multiple times.\n",
    "\n",
    "\n",
    "Finally, as you derive new RDDs from each other using transformations, Spark keeps track of the set of dependencies between different RDDs, called the lineage graph. For instance, the code bellow corresponds to the following graph:\n",
    "\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/BiRkaa97xKEZ7NE/download\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 276.652099609375,
      "end_time": 1.60457208264972E12
     }
    },
    "id": "Tg8ZBO5L2E88",
    "outputId": "3a0f651c-e5a7-4a9b-f51f-12e10884bb5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'pineapple', 'lemon']\n",
      "(2) UnionRDD[36] at union at NativeMethodAccessorImpl.java:0 []\n",
      " |  PythonRDD[34] at RDD at PythonRDD.scala:53 []\n",
      " |  fruits.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  fruits.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  PythonRDD[35] at RDD at PythonRDD.scala:53 []\n",
      " |  yellowthings.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  yellowthings.txt HadoopRDD[2] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "apples = fruits.filter(lambda x: \"apple\" in x)\n",
    "lemons = yellowThings.filter(lambda x: \"lemon\" in x)\n",
    "applesAndLemons = apples.union(lemons)\n",
    "print(applesAndLemons.collect())\n",
    "print(applesAndLemons.toDebugString().decode(\"utf-8\")) # decode used for nice formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6jZlHQt2E8-"
   },
   "source": [
    "### 3.2 Exercise\n",
    "\n",
    "1. What does the code below do?\n",
    "1. Draw the linage graph for the code\n",
    "1. List actions and transformations used in it\n",
    "1. When are all computations executed?\n",
    "1. If we call `result.collect()` again, what will Spark do to perform the action? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1284.60205078125,
      "end_time": 1.604572161885815E12
     }
    },
    "id": "naDnHCH12E8_",
    "outputId": "5da8b1db-dd40-4c97-9bd8-7851b4b062c5"
   },
   "outputs": [],
   "source": [
    "text = sc.textFile('fruits.txt')\n",
    "words = text.flatMap(lambda x: x.split(\" \"))\n",
    "result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "result.saveAsTextFile('result.txt')\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLO-mxZJ2E9B"
   },
   "source": [
    "### 3.3 Persistence (Caching)\n",
    "\n",
    "Spark's RDDs are by default recomputed each time you run an action on\n",
    "them. If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using `RDD.persist()`. After computing it the first time, Spark will store the RDD contents in memory (partitioned across the machines in your cluster), and reuse them in future actions. Persisting RDDs on disk instead of memory is also possible.\n",
    "\n",
    "If you attempt to cache too much data to fit in memory, Spark will automatically evict old partitions using a Least Recently Used (LRU) cache policy. For the memory-only storage levels, it will recompute these partitions the next time they are accessed,\n",
    "while for the memory-and-disk ones, it will write them out to disk. In either case, this means that you don't have to worry about your job breaking if you ask Spark to cache too much data. However, caching unnecessary data can lead to eviction of useful data\n",
    "and more recomputation time. Finally, RDDs come with a method called `unpersist()` that lets you manually remove them from the cache.\n",
    "\n",
    "Please note that both `persist()` and `cache()` (which is a simple wrapper that calls `persist(storageLevel=StorageLevel.MEMORY_ONLY)` - see [here](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.cache) for details -) are lazy operations themselves. The caching operation will, in fact, only take place when the first action is called. With successive action calls, the cached RDD will be used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrhC0T7V2E9B"
   },
   "source": [
    "### 3.4 Exercise:\n",
    "1. Write some code which can benefit from caching.\n",
    "1. Where should we ask Spark to persist the RDD in Exercise 3.2 to prevent it from re-executing the code when we call `collect()` again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bTnnUpiMtE7"
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def debug(rdd):\n",
    "    print(rdd.toDebugString().decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "# Solution 1\n",
    "yellowFruits = fruits.intersection(yellowThings)\n",
    "ascii_digits = yellowFruits.map(lambda x: [list(str(ord(c))) for c in x]).flatMap(lambda x: x)\n",
    "ascii_digit_counts = ascii_digits.countByValue()\n",
    "debug(ascii_digits)\n",
    "\n",
    "# imaginaryFruits = fruits.filter(lambda x: x == \"canary melon\").collect()\n",
    "# imaginaryFruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2326.470947265625,
      "end_time": 1.6045698821179E12
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "mmU2tW5W2E9D",
    "outputId": "6d273032-274a-439c-c120-f7e50d973535"
   },
   "outputs": [],
   "source": [
    "# Solution 2\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Zuey7WN2E9F"
   },
   "source": [
    "### 3.5 Working with Key/Value Pairs\n",
    "\n",
    "\n",
    "Spark provides special operations on RDDs containing key/value pairs. These RDDs\n",
    "are called *pair RDDs*. Pair RDDs are a useful building block in many programs, as\n",
    "they expose operations that allow you to act on each key in parallel or regroup data\n",
    "across the network. For example, pair RDDs have a `reduceByKey()` method that can\n",
    "aggregate data separately for each key, and a `join()` method that can merge two\n",
    "RDDs together by grouping elements with the same key. Pair RDDs are also still RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 811.181884765625,
      "end_time": 1.604570014682289E12
     }
    },
    "id": "p__T2_bF2E9G",
    "outputId": "d8bf7fa1-273c-42bd-9b7e-7b500268ae2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('key1', (8, 2)), ('key2', (3, 1)), ('key3', (12, 2))]\n",
      "(1) PythonRDD[74] at collect at <ipython-input-20-1373093d76c1>:4 []\n",
      " |  MapPartitionsRDD[73] at mapPartitions at PythonRDD.scala:145 []\n",
      " |  ShuffledRDD[72] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(1) PairwiseRDD[71] at reduceByKey at <ipython-input-20-1373093d76c1>:3 []\n",
      "    |  PythonRDD[70] at reduceByKey at <ipython-input-20-1373093d76c1>:3 []\n",
      "    |  ParallelCollectionRDD[69] at readRDDFromFile at PythonRDD.scala:274 []\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "rdd = sc.parallelize([(\"key1\", 0) ,(\"key2\", 3),(\"key1\", 8) ,(\"key3\", 3),(\"key3\", 9)])\n",
    "rdd2 = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "print(rdd2.collect())\n",
    "print(rdd2.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSGny_-F2E9I"
   },
   "source": [
    "### 3.6 Exercise\n",
    "1. What does the code above do? \n",
    "2. Where can it be used? Complete the code to perform the desired functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1054.051025390625,
      "end_time": 1.604570191639609E12
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "4Sbh0ffn2E9J",
    "outputId": "16e171ea-d26c-4f4b-a868-e980b5f54a0e"
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"key1\", 0) ,(\"key2\", 3),(\"key1\", 8) ,(\"key3\", 3),(\"key3\", 9)])\n",
    "rdd2 = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "average = rdd2.mapValues(lambda x: x[0] / x[1])\n",
    "print(average.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-kni-P32E9L"
   },
   "source": [
    "### 3.7 Spark Partitioning\n",
    "Spark programs can choose to control their RDDs' partitioning\n",
    "to reduce communication. Partitioning will not be helpful in all applications, for\n",
    "example, if a given RDD is scanned only once, there is no point in partitioning it in\n",
    "advance. It is useful only when a dataset is reused multiple times in key-oriented\n",
    "operations such as joins.\n",
    "\n",
    "Spark's partitioning is available on all RDDs of key/value pairs, and causes the system\n",
    "to group elements based on a function of each key. Although Spark does not give\n",
    "explicit control of which worker node each key goes to (partly because the system is\n",
    "designed to work even if specific nodes fail), it lets the program ensure that a set of\n",
    "keys will appear together on some node.\n",
    "\n",
    "\n",
    "Many of Spark's operations involve shuffling data by key across the network. All of\n",
    "these will benefit from partitioning. Examples of operations that benefit from\n",
    "partitioning are `cogroup()`, `groupWith()`, `join()`, `leftOuterJoin()`, `rightOuterJoin()`, `groupByKey()`, `reduceByKey()`, `combineByKey()`, and `lookup()`.\n",
    "\n",
    "By default PySpark uses hash partitioning as the partitioning function. A way to define a custom partition is by using the function `partitionBy()`. To use `partitionBy()` the RDD must consist of tuple objects. This function is a transformation, therefore a new RDD will be returned. In the following example we are going to see a default partitioning scheme of Spark as well as a custom partitioning.\n",
    "\n",
    "Partitioning allows some Spark code to run more efficiently, in particular running 'pair' operations on pair RDD (eg. mapValues, reduceByKey) is guaranteed to produce no shuffling in the cluster and also preserve the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 102.837890625,
      "end_time": 1.60457060030003E12
     }
    },
    "id": "Wzb53P3K2E9L"
   },
   "outputs": [],
   "source": [
    "nums = [(1, 1), (2, 2), (3, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 94.587890625,
      "end_time": 1.604570600597958E12
     }
    },
    "id": "9RD8T7Eo2E9O"
   },
   "outputs": [],
   "source": [
    "pairs = sc.parallelize(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 296.9658203125,
      "end_time": 1.604570604299245E12
     }
    },
    "id": "AavpwTqr2E9P",
    "outputId": "8c34d7aa-c04d-40d8-9dd4-2d59331a8547"
   },
   "outputs": [],
   "source": [
    "print(\"Number of partitions: {}\".format(pairs.getNumPartitions()))\n",
    "print(\"Partitions structure: {}\".format(pairs.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqE_0i692E9R"
   },
   "source": [
    "Let's try to define a custom partitioning now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 301.080078125,
      "end_time": 1.604570637324732E12
     }
    },
    "id": "GVkt8g302E9R"
   },
   "outputs": [],
   "source": [
    "pairs = sc.parallelize(nums).partitionBy(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 791.9169921875,
      "end_time": 1.604570650694457E12
     }
    },
    "id": "z0QzAt302E9U",
    "outputId": "68b802a5-9765-46a7-cb21-ed15bfd7faf4"
   },
   "outputs": [],
   "source": [
    "print(\"Number of partitions: {}\".format(pairs.getNumPartitions()))\n",
    "print(\"Partitions structure: {}\".format(pairs.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZCEMl182E9V"
   },
   "source": [
    "### 3.8 Converting a user program into tasks\n",
    "\n",
    "A Spark driver is responsible for converting a user program into units of physical execution called tasks. At a high level, all Spark programs follow the same structure: they create RDDs from some input, derive new RDDs from those using transformations, and perform actions to collect or save data. A Spark program implicitly creates a logical **directed acyclic graph (DAG)** of operations.\n",
    "When the driver runs, it converts this logical graph into a physical execution plan.\n",
    "\n",
    "Spark performs several optimizations, such as \"pipelining\" map transformations together to merge them, and converts the execution graph into a set of **stages**.\n",
    "Each stage, in turn, consists of multiple tasks. The tasks are bundled up and prepared to be sent to the cluster. Tasks are the smallest unit of work in Spark; a typical user program can launch hundreds or thousands of individual tasks.\n",
    "\n",
    "Each RDD maintains a pointer to one or more parents along with metadata about what\n",
    "type of relationship they have. For instance, when you call `val b = a.map()` on an\n",
    "RDD, the RDD `b` keeps a reference to its parent `a`. These pointers allow an RDD to be\n",
    "traced to all of its ancestors.\n",
    "\n",
    "The following phases occur during Spark execution:\n",
    "* User code defines a DAG (directed acyclic graph) of RDDs. Operations on RDDs create new RDDs that refer back to their parents, thereby creating a graph.\n",
    "* Actions force translation of the DAG to an execution plan. When you call an action on an RDD, it must be computed. This requires computing its parent RDDs as well. \n",
    "* Spark's scheduler submits a job to compute all needed RDDs. That job will have one or more stages, which are parallel waves of computation composed of tasks. Each stage will correspond to one or more RDDs in the DAG. A single stage can correspond to multiple RDDs due to pipelining.\n",
    "* Tasks are scheduled and executed on a cluster\n",
    "* Stages are processed in order, with individual tasks launching to compute segments of the RDD. Once the final stage is finished in a job, the action is complete.\n",
    "\n",
    "If you visit the application's web UI (http://localhost:4040/jobs/), you will see how many stages occur in order to\n",
    "fulfill an action. For more details about the content of this page, see [Spark job debugging](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-spark-job-debugging) for Azure Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FD9QH0PY2E9W"
   },
   "source": [
    "### 3.9 Exercise. \n",
    "\n",
    "1. Why is Spark faster than Hadoop MapReduce?\n",
    "1. Study the examples above via Spark UI. Observe how many stages they have. \n",
    "1. Which of the graphs below are DAGs?\n",
    "\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/kkes72cJjsoHkbY/download\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hH7j0cAO2E9W"
   },
   "source": [
    "### 3.10 True or False\n",
    "Say if the following statements are *true* or *false*, and explain why.\n",
    "\n",
    "1. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster.\n",
    "1. Transformations construct a new RDD from a previous one and immediately calculate the result\n",
    "1. Spark's RDDs are by default recomputed each time you run an action on them\n",
    "1. After computing an RDD, Spark will store its contents in memory and reuse them in future actions.\n",
    "1. When you derive new RDDs using transformations, Spark keeps track of the set of dependencies between different RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Th3qkpTo2E9X"
   },
   "source": [
    "## 4. TF-IDF in Spark (OPTIONAL)\n",
    "In this exercise you will implement a simple query engine over the Gutenberg dataset using Spark.\n",
    "The [Gutenberg dataset](https://www.gutenberg.org/) consists of 3036 free ebooks. The goal of this exercise is to develop a search engine to find the most relevant books given a text query.\n",
    "\n",
    "### 4.1 Get the data\n",
    "1. You can download the dataset from:<br>\n",
    "    wget \"https://cloud.inf.ethz.ch/s/mNPejXzsTCqKYnd/download/gutenberg.tar.gz\"\n",
    "\n",
    "2. Untar the data\n",
    "\n",
    "3. Copy the dataset to hdfs:<br>\n",
    "    docker cp gutenberg jupyter:/home/jovyan/work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kxrDBCi2E9X"
   },
   "source": [
    "### 4.2 Understand TF-IDF\n",
    "\n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a statistic to determine the relative importance of the words in a set of documents. Is is computed as the product of two statistics, term frequency (`tf`) and inverse document frequency (`idf`). \n",
    "\n",
    "Given a word `t`, a document `d` (in this case, a book) and the collection of all documents `D` we can define `tf(t, d)` as the number of times `t` appears in `d`. This gives us some information about the content of a document but because some terms (eg. \"the\") are so common, term frequency will tend to incorrectly emphasize documents which happen to use the word \"the\" more frequently, without giving enough weight to the more meaningful terms.\n",
    "\n",
    "The inverse document frequency `idf(t, D)` is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It can be computed as:\n",
    "\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/gw25WWcbd9iXBdK/download\" width=\"300\">\n",
    "\n",
    "where $|D|$ is the total number of documents and the denominator represents how many documents contain the word $t$ at least once. However, this would cause a division-by-zero exception if the user query a word that never appear in the dataset. A better formulation would be:\n",
    "\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/fXffB5g59y3y2an/download\" width=\"300\">\n",
    "\n",
    "Then, the `tdidf(t, d, D)` is calculated as follows:\n",
    "\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/2dAsg3k2QaL3XMz/download\" width=\"300\">\n",
    "\n",
    "A high weight in `tfidf` is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents.\n",
    "\n",
    "Having already implemented TF-IDF last week in pseudocode, in this week we are going to implement it in Spark. The following code snippet imports the whole dataset into an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 362.551025390625,
      "end_time": 1.604572323489335E12
     }
    },
    "id": "cBgZ_ztV2E9X"
   },
   "outputs": [],
   "source": [
    "# sc is automatically defined as SparkContext\n",
    "# docs will be an RDD in the format [(docName, content)]\n",
    "docs = sc.wholeTextFiles(\"gutenberg/*.txt\", minPartitions=100)\n",
    "\n",
    "# number of documents in the folder\n",
    "docs_number = docs.count()\n",
    "\n",
    "# display the [(docName, content)] values\n",
    "docs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnD0z1DA2E9Z"
   },
   "source": [
    "#### TF-IDF solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "8r-Wtteo2E8d",
    "K9y3brO92E8d",
    "oNbMFPQm2E8d",
    "KEb2NxyP2E8e",
    "QLO-mxZJ2E9B",
    "xZCEMl182E9V",
    "FD9QH0PY2E9W",
    "EPnOr09W2E9W"
   ],
   "name": "Exercise08_Spark_Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}